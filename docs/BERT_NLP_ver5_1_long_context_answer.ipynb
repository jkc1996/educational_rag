{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e59f37021abf49dd8d24e3ae67576299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98db57a310804c66aa1bcff2df45e659",
              "IPY_MODEL_b5c862e6265c4fb4b7633eea116c0f90",
              "IPY_MODEL_5dcb7e39f39349e887a7a653b2dce16d"
            ],
            "layout": "IPY_MODEL_d936ebda82264f06bd382f1b8b2c5388"
          }
        },
        "98db57a310804c66aa1bcff2df45e659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2132ea67776949578236ec73ef4ade90",
            "placeholder": "​",
            "style": "IPY_MODEL_835f67feb41f46f1bc339a5dc97bf4ef",
            "value": "Batches: 100%"
          }
        },
        "b5c862e6265c4fb4b7633eea116c0f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_266dd757490b408090abf0a31610ccf7",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b53ed98f2fb44c628d5f3cfaa95d0ce1",
            "value": 5
          }
        },
        "5dcb7e39f39349e887a7a653b2dce16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3650d0b215fe42af80da76c9003a77e2",
            "placeholder": "​",
            "style": "IPY_MODEL_b35adf4ff00c4b178ea7f02889d6460e",
            "value": " 5/5 [00:09&lt;00:00,  1.37s/it]"
          }
        },
        "d936ebda82264f06bd382f1b8b2c5388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2132ea67776949578236ec73ef4ade90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "835f67feb41f46f1bc339a5dc97bf4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "266dd757490b408090abf0a31610ccf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b53ed98f2fb44c628d5f3cfaa95d0ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3650d0b215fe42af80da76c9003a77e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b35adf4ff00c4b178ea7f02889d6460e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Required Library Installation**"
      ],
      "metadata": {
        "id": "I7z5di6kLeLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx sentence-transformers faiss-cpu transformers --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0rBKX6akLBo",
        "outputId": "298190db-d64a-4a15-cd18-3eb1dfe3660e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Document loading and Paragraph Extraction**"
      ],
      "metadata": {
        "id": "ilVplhH8zOaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "\n",
        "# Load your handout\n",
        "doc = Document('ML_course_content_1.docx')\n",
        "paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
        "\n",
        "# Join all paragraphs into one big text\n",
        "handout_text = \"\\n\".join(paras)\n",
        "documents = [handout_text]\n"
      ],
      "metadata": {
        "id": "YWZhVEuJkL33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hybrid chunking**"
      ],
      "metadata": {
        "id": "aFx2KQa_zOxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# 'punkt' and 'punkt_tab' for sentence tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def hybrid_chunking(paras, min_words=40, max_words=200):\n",
        "    chunks = []\n",
        "    buffer = []\n",
        "    buffer_len = 0\n",
        "\n",
        "    for para in paras:\n",
        "        para = para.strip()\n",
        "        if not para:\n",
        "            continue\n",
        "        para_words = para.split()\n",
        "        n_words = len(para_words)\n",
        "\n",
        "        # Case 1: Paragraph is too long, split by sentences\n",
        "        if n_words > max_words:\n",
        "            sentences = sent_tokenize(para)\n",
        "            sent_buffer = []\n",
        "            sent_count = 0\n",
        "            for sent in sentences:\n",
        "                sent_words = sent.split()\n",
        "                sent_buffer += sent_words\n",
        "                sent_count += len(sent_words)\n",
        "                if sent_count >= min_words:\n",
        "                    chunks.append(' '.join(sent_buffer))\n",
        "                    sent_buffer = []\n",
        "                    sent_count = 0\n",
        "            if sent_buffer:\n",
        "                chunks.append(' '.join(sent_buffer))\n",
        "            continue\n",
        "\n",
        "        # Case 2: Paragraph is short, merge into buffer\n",
        "        buffer += para_words\n",
        "        buffer_len += n_words\n",
        "        if buffer_len >= min_words:\n",
        "            chunks.append(' '.join(buffer))\n",
        "            buffer = []\n",
        "            buffer_len = 0\n",
        "\n",
        "    # Flush buffer\n",
        "    if buffer:\n",
        "        chunks.append(' '.join(buffer))\n",
        "    return chunks\n",
        "\n",
        "# Usage with your docx paragraphs:\n",
        "from docx import Document\n",
        "\n",
        "doc = Document('ML_course_content_1.docx')\n",
        "paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
        "chunk_texts = hybrid_chunking(paras, min_words=40, max_words=120)\n",
        "\n",
        "print(f\"Created {len(chunk_texts)} hybrid chunks.\")\n",
        "for i, ch in enumerate(chunk_texts[:5]):\n",
        "    print(f\"\\nChunk {i+1} ({len(ch.split())} words):\\n{ch}\\n{'='*40}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaO1msmqkPiv",
        "outputId": "e197c4f9-41b5-4214-faba-71b08e8905d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 143 hybrid chunks.\n",
            "\n",
            "Chunk 1 (68 words):\n",
            "1.1 Introduction to ML Machine Learning (ML) is a subfield of artificial intelligence that focuses on designing algorithms capable of learning from data and improving over time without being explicitly programmed for every possible task. Unlike traditional software, where rules are crafted by human programmers, ML systems identify and extract patterns from large volumes of data, allowing them to make predictions, detect anomalies, and even generate new content.\n",
            "========================================\n",
            "\n",
            "Chunk 2 (49 words):\n",
            "<br> The applications of ML are vast: self-driving cars, language translation, recommendation engines, medical image analysis, fraud detection, and more. ML approaches include supervised learning (with labeled data), unsupervised learning (discovering structure in unlabeled data), semi-supervised learning (combining both), and reinforcement learning (learning via trial and error and rewards).\n",
            "========================================\n",
            "\n",
            "Chunk 3 (31 words):\n",
            "<br> The success of modern ML stems from increased computational power, larger and richer datasets, and algorithmic innovations. However, effective ML also depends on data quality, model selection, and robust evaluation.\n",
            "========================================\n",
            "\n",
            "Chunk 4 (44 words):\n",
            "1.2 Objective of the Course The purpose of this course is to help students develop both a theoretical and practical understanding of the key principles underlying machine learning. Students will gain hands-on experience with real-world datasets, working through preprocessing, model building, evaluation, and deployment.\n",
            "========================================\n",
            "\n",
            "Chunk 5 (66 words):\n",
            "The curriculum is designed to foster critical thinking about tradeoffs such as model complexity versus interpretability, the impact of data quality, and ethical considerations in automated decision making. <br> By the end of this course, students will be able to implement, train, tune, and validate a range of ML models (including regression, classification, and clustering) and apply them to challenging problems in science, engineering, and business.\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute Embeddings for Chunks**"
      ],
      "metadata": {
        "id": "EZzwCviSzU2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(chunk_texts, show_progress_bar=True)\n",
        "\n",
        "embeddings = np.array(embeddings).astype('float32')\n",
        "faiss.normalize_L2(embeddings)\n",
        "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# peaking into embeddings\n",
        "\n",
        "print(\"Embedding shape:\", embeddings.shape)\n",
        "print(\"First chunk embedding (first 10 dims):\", embeddings[0][:10])\n",
        "print(\"First 3 embeddings:\\n\", embeddings[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "e59f37021abf49dd8d24e3ae67576299",
            "98db57a310804c66aa1bcff2df45e659",
            "b5c862e6265c4fb4b7633eea116c0f90",
            "5dcb7e39f39349e887a7a653b2dce16d",
            "d936ebda82264f06bd382f1b8b2c5388",
            "2132ea67776949578236ec73ef4ade90",
            "835f67feb41f46f1bc339a5dc97bf4ef",
            "266dd757490b408090abf0a31610ccf7",
            "b53ed98f2fb44c628d5f3cfaa95d0ce1",
            "3650d0b215fe42af80da76c9003a77e2",
            "b35adf4ff00c4b178ea7f02889d6460e"
          ]
        },
        "id": "R3tTZZeqzayX",
        "outputId": "a64ca1df-74b3-47e9-f6de-7cf20976ebac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e59f37021abf49dd8d24e3ae67576299"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: (143, 384)\n",
            "First chunk embedding (first 10 dims): [-0.04891609 -0.03908867  0.04322965  0.00759483  0.02771054 -0.0615214\n",
            " -0.00089626 -0.06041696 -0.05594254 -0.01826064]\n",
            "First 3 embeddings:\n",
            " [[-0.04891609 -0.03908867  0.04322965 ...  0.09220957  0.04523145\n",
            "  -0.05161026]\n",
            " [-0.0522799  -0.04477102  0.013062   ...  0.00473073  0.01609313\n",
            "  -0.04416708]\n",
            " [-0.00738526 -0.08043031  0.02125871 ... -0.05418754  0.03361555\n",
            "  -0.03006891]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build FAISS Index and Search**"
      ],
      "metadata": {
        "id": "ahbkVOq_kXGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"which is the most widely used linear classification model?\"\n",
        "q_emb = model.encode([query]).astype('float32')\n",
        "faiss.normalize_L2(q_emb)\n",
        "top_k = 10  # Retrieve more chunks for context expansion!\n",
        "D, I = index.search(q_emb, top_k)\n",
        "\n",
        "retrieved_chunks = [chunk_texts[idx] for idx in I[0]]\n",
        "print(\"Top retrieved chunks for context expansion:\\n\")\n",
        "for rank, chunk in enumerate(retrieved_chunks):\n",
        "    print(f\"Rank {rank+1}: (Score: {D[0][rank]:.4f})\")\n",
        "    print(chunk)\n",
        "    print('-' * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_tfuZJfzMEE",
        "outputId": "29b7d60f-c81e-4490-fbd3-99a3587e6be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top retrieved chunks for context expansion:\n",
            "\n",
            "Rank 1: (Score: 0.6250)\n",
            "Imagine you have a scatter plot of data points, and your goal is to draw a straight line (or a flat plane in higher dimensions) that perfectly divides these points into distinct categories. This simple yet powerful idea is the essence of linear models for classification. These models are fundamental to machine learning because they offer a straightforward, interpretable, and computationally efficient way to categorize data.\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 2: (Score: 0.6003)\n",
            "The primary advantages of linear models are their simplicity, making them easy to understand and implement; their interpretability, as you can see which features contribute most to the classification based on their weights; and their speed, making them suitable for large datasets. They serve as an excellent baseline for many real-world classification problems.\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 3: (Score: 0.5868)\n",
            "3.4 Logistic Regression Logistic Regression is one of the most fundamental and widely used linear classification algorithms in machine learning. Despite the word \"regression\" in its name, it is primarily used for binary classification, predicting whether an input belongs to one of two classes (e.g., \"yes\" or \"no,\" \"true\" or \"false,\" \"spam\" or \"not spam\"). Its strength lies in its ability to output probabilities for class membership, rather than just a hard binary label.\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 4: (Score: 0.5714)\n",
            "Let's delve into how some prominent linear models leverage this concept: 1. The Perceptron: The Original Learner The Perceptron, invented by Frank Rosenblatt in the late 1950s, is a historical cornerstone in machine learning and the simplest type of artificial neural network. It's a binary classifier, meaning it can only distinguish between two classes (e.g., \"yes\" or \"no\").\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 5: (Score: 0.5712)\n",
            "Low Bias, Low Variance: Ideal but difficult to achieve. High Bias, Low Variance: Underfits. Low Bias, High Variance: Overfits. Practically, techniques such as cross-validation, regularization, and ensembling (e.g., bagging, boosting) help find and maintain this trade-off. 3. Linear Models for Classification\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 6: (Score: 0.5212)\n",
            "Model Selection: Choosing the right learning algorithm is crucial. Options range from simple linear regression to decision trees, SVMs, ensemble methods, or neural networks. Model selection may depend on the data type, problem complexity, interpretability needs, and computational constraints. Training the Model: Training involves feeding the model with data and adjusting its parameters to minimize a loss function—such as mean squared error for regression, or cross-entropy for classification. Optimization techniques (like gradient descent and its variants) play a central role here.\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 7: (Score: 0.4906)\n",
            "3. Support Vector Machines (Linear SVM): The Widest Margin Wins The Linear Support Vector Machine (SVM) takes a sophisticated approach to defining the separating line. Instead of just finding any line that separates the classes (like the Perceptron might), the SVM aims to find the unique line that maximizes the margin between the classes.\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 8: (Score: 0.4901)\n",
            "The most prominent example of a probabilistic discriminative linear classifier is Logistic Regression, which directly outputs probabilities of class membership. Other advanced examples include Conditional Random Fields (CRFs), often used for sequence labeling tasks in natural language processing. The principles of discriminative modeling are also foundational to the design of many modern neural networks used for classification, where the network learns complex mappings from inputs directly to class probabilities. These models are highly valuable because they provide confidence scores along with predictions, allowing for nuanced decision-making in applications like medical diagnosis or financial risk assessment.\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 9: (Score: 0.4896)\n",
            "Assumptions: While robust, it assumes a linear relationship between the input features and the log-odds of the outcome, which might not always hold perfectly. Logistic Regression remains a cornerstone in machine learning. It's often the first model to try for classification tasks due to its balance of simplicity, performance, and crucial probabilistic outputs, making it widely applicable in various fields, from healthcare to finance and social sciences.\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 10: (Score: 0.4753)\n",
            "2. Fisher's Linear Discriminant Analysis (LDA): Maximizing Separation Unlike the Perceptron, which is a purely \"discriminative\" model (focusing only on the boundary), LDA has roots in generative modeling (it assumes how data is generated). However, it results in a powerful linear discriminant for classification.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# BERT-based QA Pipeline\n",
        "\n",
        "qa = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"bert-large-uncased-whole-word-masking-finetuned-squad\",  # BERT SQuAD v1.1 model\n",
        "    tokenizer=\"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Gkr8sV1ZApu",
        "outputId": "2f0548d1-6288-4862-fc3c-6e787f848978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def get_best_sentence_from_topk(chunk_texts, question, I, topk=5, min_length=10):\n",
        "    stopwords = set([\n",
        "        'what', 'is', 'the', 'a', 'an', 'of', 'in', 'on', 'and', 'or', 'for', 'to', 'with', 'by', 'it', 'as', 'that',\n",
        "        'this', 'from', 'at', 'which', 'are', 'be', 'was', 'were', 'has', 'have', 'had', 'but', 'not', 'all', 'so', 'should'\n",
        "    ])\n",
        "    q_words = set([w.lower() for w in re.findall(r'\\w+', question) if w.lower() not in stopwords])\n",
        "    best_score = 0\n",
        "    best_sentence = \"\"\n",
        "    for idx in I[0][:topk]:\n",
        "        chunk = chunk_texts[idx]\n",
        "        sentences = re.split(r'(?<=[.!?]) +', chunk)\n",
        "        for sent in sentences:\n",
        "            s_words = set([w.lower() for w in re.findall(r'\\w+', sent)])\n",
        "            score = len(q_words.intersection(s_words))\n",
        "            if score > best_score and len(sent.split()) >= min_length:\n",
        "                best_score = score\n",
        "                best_sentence = sent.strip()\n",
        "    return best_sentence\n",
        "\n",
        "def smart_context_qa_best_global(\n",
        "    qa_pipeline, query, chunk_texts, I,\n",
        "    min_length=10, min_score=0.2, expand_topk=5):\n",
        "\n",
        "    # Try QA model on top-1 chunk\n",
        "    context = chunk_texts[I[0][0]]\n",
        "    result = qa_pipeline(question=query, context=context)\n",
        "    answer = result['answer'].strip()\n",
        "    score = result['score']\n",
        "    nwords = len(answer.split())\n",
        "    print(\"Answer given by pipeline(Before applying any fallback conditions):\\n\", answer);\n",
        "\n",
        "    # If QA answer is good, use it\n",
        "    if nwords >= min_length and score >= min_score:\n",
        "        print(\"Extracted answer from QA model:\\n\", answer)\n",
        "        return answer\n",
        "\n",
        "    # If not, run global keyword search on top-K chunks\n",
        "    best_sentence = get_best_sentence_from_topk(chunk_texts, query, I, topk=expand_topk, min_length=min_length)\n",
        "    if best_sentence:\n",
        "        print(\"Answer by global keyword-matched sentence:\\n\", best_sentence)\n",
        "        return best_sentence\n",
        "\n",
        "    # Fallback strategy\n",
        "    fallback_sent = re.split(r'(?<=[.!?]) +', chunk_texts[I[0][0]])[0].strip()\n",
        "    print(\"Fallback: First sentence from top chunk:\\n\", fallback_sent)\n",
        "    return fallback_sent\n",
        "\n",
        "# Usage\n",
        "smart_context_qa_best_global(qa, query, chunk_texts, I)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "K3hZV-1osfoT",
        "outputId": "1009410e-fead-44c6-ac87-21d41cfbcb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer given by pipeline(Before applying any fallback conditions):\n",
            " linear models for classification\n",
            "Answer by global keyword-matched sentence:\n",
            " 3.4 Logistic Regression Logistic Regression is one of the most fundamental and widely used linear classification algorithms in machine learning.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.4 Logistic Regression Logistic Regression is one of the most fundamental and widely used linear classification algorithms in machine learning.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}