# -*- coding: utf-8 -*-
"""BERT_NLP_ver5.1_long_context_answer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I4x-e4jY3xEl2HXOyDnKf5pnRRRWaZTf

**Required Library Installation**
"""

!pip install python-docx sentence-transformers faiss-cpu transformers --quiet

"""**Document loading and Paragraph Extraction**"""

from docx import Document

# Load your handout
doc = Document('ML_course_content_1.docx')
paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]

# Join all paragraphs into one big text
handout_text = "\n".join(paras)
documents = [handout_text]

"""**Hybrid chunking**"""

import nltk
# 'punkt' and 'punkt_tab' for sentence tokenization
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.tokenize import sent_tokenize

def hybrid_chunking(paras, min_words=40, max_words=200):
    chunks = []
    buffer = []
    buffer_len = 0

    for para in paras:
        para = para.strip()
        if not para:
            continue
        para_words = para.split()
        n_words = len(para_words)

        # Case 1: Paragraph is too long, split by sentences
        if n_words > max_words:
            sentences = sent_tokenize(para)
            sent_buffer = []
            sent_count = 0
            for sent in sentences:
                sent_words = sent.split()
                sent_buffer += sent_words
                sent_count += len(sent_words)
                if sent_count >= min_words:
                    chunks.append(' '.join(sent_buffer))
                    sent_buffer = []
                    sent_count = 0
            if sent_buffer:
                chunks.append(' '.join(sent_buffer))
            continue

        # Case 2: Paragraph is short, merge into buffer
        buffer += para_words
        buffer_len += n_words
        if buffer_len >= min_words:
            chunks.append(' '.join(buffer))
            buffer = []
            buffer_len = 0

    # Flush buffer
    if buffer:
        chunks.append(' '.join(buffer))
    return chunks

# Usage with your docx paragraphs:
from docx import Document

doc = Document('ML_course_content_1.docx')
paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]
chunk_texts = hybrid_chunking(paras, min_words=40, max_words=120)

print(f"Created {len(chunk_texts)} hybrid chunks.")
for i, ch in enumerate(chunk_texts[:5]):
    print(f"\nChunk {i+1} ({len(ch.split())} words):\n{ch}\n{'='*40}")

"""**Compute Embeddings for Chunks**"""

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(chunk_texts, show_progress_bar=True)

embeddings = np.array(embeddings).astype('float32')
faiss.normalize_L2(embeddings)
index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings)

# peaking into embeddings

print("Embedding shape:", embeddings.shape)
print("First chunk embedding (first 10 dims):", embeddings[0][:10])
print("First 3 embeddings:\n", embeddings[:3])

"""**Build FAISS Index and Search**"""

query = "which is the most widely used linear classification model?"
q_emb = model.encode([query]).astype('float32')
faiss.normalize_L2(q_emb)
top_k = 10  # Retrieve more chunks for context expansion!
D, I = index.search(q_emb, top_k)

retrieved_chunks = [chunk_texts[idx] for idx in I[0]]
print("Top retrieved chunks for context expansion:\n")
for rank, chunk in enumerate(retrieved_chunks):
    print(f"Rank {rank+1}: (Score: {D[0][rank]:.4f})")
    print(chunk)
    print('-' * 80)

from transformers import pipeline

# BERT-based QA Pipeline

qa = pipeline(
    "question-answering",
    model="bert-large-uncased-whole-word-masking-finetuned-squad",  # BERT SQuAD v1.1 model
    tokenizer="bert-large-uncased-whole-word-masking-finetuned-squad"
)

import re
from collections import Counter

def get_best_sentence_from_topk(chunk_texts, question, I, topk=5, min_length=10):
    stopwords = set([
        'what', 'is', 'the', 'a', 'an', 'of', 'in', 'on', 'and', 'or', 'for', 'to', 'with', 'by', 'it', 'as', 'that',
        'this', 'from', 'at', 'which', 'are', 'be', 'was', 'were', 'has', 'have', 'had', 'but', 'not', 'all', 'so', 'should'
    ])
    q_words = set([w.lower() for w in re.findall(r'\w+', question) if w.lower() not in stopwords])
    best_score = 0
    best_sentence = ""
    for idx in I[0][:topk]:
        chunk = chunk_texts[idx]
        sentences = re.split(r'(?<=[.!?]) +', chunk)
        for sent in sentences:
            s_words = set([w.lower() for w in re.findall(r'\w+', sent)])
            score = len(q_words.intersection(s_words))
            if score > best_score and len(sent.split()) >= min_length:
                best_score = score
                best_sentence = sent.strip()
    return best_sentence

def smart_context_qa_best_global(
    qa_pipeline, query, chunk_texts, I,
    min_length=10, min_score=0.2, expand_topk=5):

    # Try QA model on top-1 chunk
    context = chunk_texts[I[0][0]]
    result = qa_pipeline(question=query, context=context)
    answer = result['answer'].strip()
    score = result['score']
    nwords = len(answer.split())
    print("Answer given by pipeline(Before applying any fallback conditions):\n", answer);

    # If QA answer is good, use it
    if nwords >= min_length and score >= min_score:
        print("Extracted answer from QA model:\n", answer)
        return answer

    # If not, run global keyword search on top-K chunks
    best_sentence = get_best_sentence_from_topk(chunk_texts, query, I, topk=expand_topk, min_length=min_length)
    if best_sentence:
        print("Answer by global keyword-matched sentence:\n", best_sentence)
        return best_sentence

    # Fallback strategy
    fallback_sent = re.split(r'(?<=[.!?]) +', chunk_texts[I[0][0]])[0].strip()
    print("Fallback: First sentence from top chunk:\n", fallback_sent)
    return fallback_sent

# Usage
smart_context_qa_best_global(qa, query, chunk_texts, I)